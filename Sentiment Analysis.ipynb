{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5285105a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T05:32:23.144886Z",
     "start_time": "2023-03-06T05:32:16.379135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.3.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from lightgbm) (1.9.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\admin\\anaconda3\\lib\\site-packages (from lightgbm) (0.38.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from lightgbm) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "093d6b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T05:43:50.788905Z",
     "start_time": "2023-03-06T05:41:57.811474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf03fdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T06:01:35.893399Z",
     "start_time": "2023-03-06T06:01:33.063424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement numpy==version (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0rc1, 1.23.0rc2, 1.23.0rc3, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0rc1, 1.24.0rc2, 1.24.0, 1.24.1, 1.24.2)\n",
      "ERROR: No matching distribution found for numpy==version\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3caa1f42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T06:03:32.850128Z",
     "start_time": "2023-03-06T06:02:16.585473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.5\n",
      "  Downloading numpy-1.23.5-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "     -------------------------------------- 14.7/14.7 MB 271.6 kB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-gihrp94r\\\\core\\\\_multiarray_umath.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc640e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:00:33.189272Z",
     "start_time": "2023-03-30T18:00:17.882298Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21dbbdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:05:09.164526Z",
     "start_time": "2023-03-30T18:05:08.345356Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId  \\\n",
       "0         1           1   \n",
       "1         2           1   \n",
       "2         3           1   \n",
       "3         4           1   \n",
       "4         5           1   \n",
       "5         6           1   \n",
       "6         7           1   \n",
       "7         8           1   \n",
       "8         9           1   \n",
       "9        10           1   \n",
       "\n",
       "                                                                                                                                                                                         Phrase  \\\n",
       "0  A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .   \n",
       "1                                                                                                                 A series of escapades demonstrating the adage that what is good for the goose   \n",
       "2                                                                                                                                                                                      A series   \n",
       "3                                                                                                                                                                                             A   \n",
       "4                                                                                                                                                                                        series   \n",
       "5                                                                                                                          of escapades demonstrating the adage that what is good for the goose   \n",
       "6                                                                                                                                                                                            of   \n",
       "7                                                                                                                             escapades demonstrating the adage that what is good for the goose   \n",
       "8                                                                                                                                                                                     escapades   \n",
       "9                                                                                                                                       demonstrating the adage that what is good for the goose   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  \n",
       "5          2  \n",
       "6          2  \n",
       "7          2  \n",
       "8          2  \n",
       "9          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(r'G:\\Dowloads File\\train.tsv\\train.tsv', sep=\"\\t\")\n",
    "test = pd.read_csv(r'G:\\Dowloads File\\test.tsv\\test.tsv', sep=\"\\t\")\n",
    "sub = pd.read_csv(r'G:\\Dowloads File\\sampleSubmission.csv', sep=\",\")\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a201abd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:05:19.602911Z",
     "start_time": "2023-03-30T18:05:19.527033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>This</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining independent</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>, introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>and</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>entertaining</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>independent</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>seeking</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId  \\\n",
       "63        64           2   \n",
       "64        65           2   \n",
       "65        66           2   \n",
       "66        67           2   \n",
       "67        68           2   \n",
       "68        69           2   \n",
       "69        70           2   \n",
       "70        71           2   \n",
       "71        72           2   \n",
       "72        73           2   \n",
       "73        74           2   \n",
       "74        75           2   \n",
       "75        76           2   \n",
       "76        77           2   \n",
       "77        78           2   \n",
       "78        79           2   \n",
       "79        80           2   \n",
       "80        81           2   \n",
       "\n",
       "                                                                        Phrase  \\\n",
       "63  This quiet , introspective and entertaining independent is worth seeking .   \n",
       "64                     This quiet , introspective and entertaining independent   \n",
       "65                                                                        This   \n",
       "66                          quiet , introspective and entertaining independent   \n",
       "67                                      quiet , introspective and entertaining   \n",
       "68                                                                       quiet   \n",
       "69                                            , introspective and entertaining   \n",
       "70                                              introspective and entertaining   \n",
       "71                                                           introspective and   \n",
       "72                                                               introspective   \n",
       "73                                                                         and   \n",
       "74                                                                entertaining   \n",
       "75                                                                 independent   \n",
       "76                                                          is worth seeking .   \n",
       "77                                                            is worth seeking   \n",
       "78                                                                    is worth   \n",
       "79                                                                       worth   \n",
       "80                                                                     seeking   \n",
       "\n",
       "    Sentiment  \n",
       "63          4  \n",
       "64          3  \n",
       "65          2  \n",
       "66          4  \n",
       "67          3  \n",
       "68          2  \n",
       "69          3  \n",
       "70          3  \n",
       "71          3  \n",
       "72          2  \n",
       "73          2  \n",
       "74          4  \n",
       "75          2  \n",
       "76          3  \n",
       "77          4  \n",
       "78          2  \n",
       "79          2  \n",
       "80          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.SentenceId == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69df320d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:05:24.461568Z",
     "start_time": "2023-03-30T18:05:24.343994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average count of phrases per sentence in train is 18.\n",
      "Average count of phrases per sentence in test is 20.\n"
     ]
    }
   ],
   "source": [
    "print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
    "print('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7053658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:05:25.441156Z",
     "start_time": "2023-03-30T18:05:25.327794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average count of phrases per sentence in train is 18.\n",
      "Average count of phrases per sentence in test is 20.\n"
     ]
    }
   ],
   "source": [
    "print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
    "print('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cffb496f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:05:26.789980Z",
     "start_time": "2023-03-30T18:05:26.252673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of phrases in train is 7.\n",
      "Average word length of phrases in test is 7.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a0aaa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:05:27.275516Z",
     "start_time": "2023-03-30T18:05:27.126571Z"
    }
   },
   "outputs": [],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f90f6e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:05:28.853247Z",
     "start_time": "2023-03-30T18:05:28.628893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the'), 199),\n",
       " (('of', 'the', 'year'), 103),\n",
       " (('.', 'is', 'a'), 87),\n",
       " (('of', 'the', 'best'), 80),\n",
       " (('of', 'the', 'most'), 70),\n",
       " (('is', 'one', 'of'), 50),\n",
       " (('One', 'of', 'the'), 43),\n",
       " ((',', 'and', 'the'), 40),\n",
       " (('the', 'year', \"'s\"), 38),\n",
       " (('It', \"'s\", 'a'), 38),\n",
       " (('it', \"'s\", 'a'), 37),\n",
       " (('.', \"'s\", 'a'), 37),\n",
       " (('a', 'movie', 'that'), 35),\n",
       " (('the', 'edge', 'of'), 34),\n",
       " (('the', 'kind', 'of'), 33),\n",
       " (('of', 'your', 'seat'), 33),\n",
       " (('the', 'film', 'is'), 31),\n",
       " ((',', 'this', 'is'), 31),\n",
       " (('the', 'film', \"'s\"), 31),\n",
       " ((',', 'the', 'film'), 30),\n",
       " (('film', 'that', 'is'), 30),\n",
       " (('as', 'one', 'of'), 30),\n",
       " (('edge', 'of', 'your'), 29),\n",
       " ((',', 'it', \"'s\"), 27),\n",
       " (('a', 'film', 'that'), 27),\n",
       " (('as', 'well', 'as'), 27),\n",
       " ((',', 'funny', ','), 25),\n",
       " ((',', 'but', 'it'), 23),\n",
       " (('films', 'of', 'the'), 23),\n",
       " (('some', 'of', 'the'), 23)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef94c7be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:07:00.747138Z",
     "start_time": "2023-03-30T18:05:31.009760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'funny', ','), 33),\n",
       " (('one', 'year', \"'s\"), 28),\n",
       " (('year', \"'s\", 'best'), 26),\n",
       " (('movies', 'ever', 'made'), 19),\n",
       " ((',', 'solid', 'cast'), 19),\n",
       " (('solid', 'cast', ','), 18),\n",
       " ((\"'ve\", 'ever', 'seen'), 16),\n",
       " (('.', 'It', \"'s\"), 16),\n",
       " ((',', 'making', 'one'), 15),\n",
       " (('best', 'films', 'year'), 15),\n",
       " ((',', 'touching', ','), 15),\n",
       " (('exquisite', 'acting', ','), 15),\n",
       " (('acting', ',', 'inventive'), 14),\n",
       " ((',', 'inventive', 'screenplay'), 14),\n",
       " (('jaw-dropping', 'action', 'sequences'), 14),\n",
       " (('good', 'acting', ','), 14),\n",
       " ((\"'s\", 'best', 'films'), 14),\n",
       " (('I', \"'ve\", 'seen'), 14),\n",
       " (('funny', ',', 'even'), 14),\n",
       " (('best', 'war', 'movies'), 13),\n",
       " (('purely', 'enjoyable', 'satisfying'), 13),\n",
       " (('funny', ',', 'touching'), 13),\n",
       " ((',', 'smart', ','), 13),\n",
       " (('inventive', 'screenplay', ','), 13),\n",
       " (('funniest', 'jokes', 'movie'), 13),\n",
       " (('action', 'sequences', ','), 13),\n",
       " (('sequences', ',', 'striking'), 13),\n",
       " ((',', 'striking', 'villains'), 13),\n",
       " (('exquisite', 'motion', 'picture'), 13),\n",
       " (('war', 'movies', 'ever'), 12)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
    "text_trigrams = [i for i in ngrams(text, 3)]\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76ea1fdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:07:00.771336Z",
     "start_time": "2023-03-30T18:07:00.755431Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "898f99f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:46:24.728075Z",
     "start_time": "2023-03-30T18:45:45.829513Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized = vectorizer.transform(train['Phrase'])\n",
    "test_vectorized = vectorizer.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aac0b08e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:46:50.558646Z",
     "start_time": "2023-03-30T18:46:50.538701Z"
    }
   },
   "outputs": [],
   "source": [
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d3b8ee8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:46:51.406159Z",
     "start_time": "2023-03-30T18:46:51.386176Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bec80a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:47:18.731015Z",
     "start_time": "2023-03-30T18:46:52.254131Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ovr.fit(train_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c00a4f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T18:47:41.733603Z",
     "start_time": "2023-03-30T18:47:18.738997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.52%, std 0.69.\n",
      "Wall time: 23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = LinearSVC(dual=False)\n",
    "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79229f52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T09:58:38.575525Z",
     "start_time": "2023-03-12T09:57:48.108871Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "ovr.fit(train_vectorized, y);\n",
    "svc.fit(train_vectorized, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef6db41e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T09:58:48.357901Z",
     "start_time": "2023-03-12T09:58:38.578775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pad-sequences>=0.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"pad-sequences>=0.6.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00f56600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T08:05:24.272103Z",
     "start_time": "2023-03-06T08:03:47.182515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.11\n",
      "  Using cached tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Using cached tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (23.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (0.31.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (1.15.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (2.11.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (3.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (0.2.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (4.5.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (3.19.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (1.51.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (23.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (67.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (1.16.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (15.0.6.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (2.2.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11) (1.23.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow==2.11) (0.38.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (2.28.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (0.6.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (0.2.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (4.9)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (2022.9.14)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11) (3.2.2)\n",
      "Installing collected packages: cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 google-auth-2.16.2 google-auth-oauthlib-0.4.6 requests-oauthlib-1.3.1 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-intel-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccac482d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:35:25.242680Z",
     "start_time": "2023-03-08T05:35:08.996919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pad-sequences>=0.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"pad-sequences>=0.6.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10aa66fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:00:05.692411Z",
     "start_time": "2023-03-30T19:00:05.657483Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "#from keras.engine import InputSpec, Layer\n",
    "from tensorflow.keras.layers import InputSpec\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b9667a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:00:13.601309Z",
     "start_time": "2023-03-30T19:00:11.310553Z"
    }
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c369181",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:00:16.242828Z",
     "start_time": "2023-03-30T19:00:14.332484Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20db5138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:00:17.863991Z",
     "start_time": "2023-03-30T19:00:16.961934Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e66932df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:06:10.341706Z",
     "start_time": "2023-03-30T19:06:10.330737Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_path = r\"C:\\Users\\Admin\\Desktop\\crawl-300d-2M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16663cb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:06:11.188412Z",
     "start_time": "2023-03-30T19:06:11.168466Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01782f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:06:11.875974Z",
     "start_time": "2023-03-30T19:06:11.818132Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Admin\\\\Desktop\\\\crawl-300d-2M'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18256\\4142537841.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_coefs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0membedding_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_coefs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mword_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnb_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Admin\\\\Desktop\\\\crawl-300d-2M'"
     ]
    }
   ],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "748034a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T12:50:28.305709Z",
     "start_time": "2023-03-12T12:50:28.265812Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "97b8487a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T12:50:31.595519Z",
     "start_time": "2023-03-12T12:50:31.548577Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c03cf00b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T12:50:33.131906Z",
     "start_time": "2023-03-12T12:50:33.093375Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModelCheckpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5936\\3100424140.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspatial_dr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5936\\3945204366.py\u001b[0m in \u001b[0;36mbuild_model1\u001b[1;34m(lr, lr_d, units, spatial_dr, kernel_size1, kernel_size2, dense_units, dr, conv_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_model1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspatial_dr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"best_model.hdf5\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n\u001b[0m\u001b[0;32m      4\u001b[0m                                   save_best_only = True, mode = \"min\")\n\u001b[0;32m      5\u001b[0m     \u001b[0mearly_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"min\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ModelCheckpoint' is not defined"
     ]
    }
   ],
   "source": [
    "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3cb3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
